{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Preprocessing import *\n",
    "from lightgbm import LGBMRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, ReLU\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, ReLU, BatchNormalization, LeakyReLU\n",
    "import string\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train, train_size=0.8, random_state=42)\n",
    "\n",
    "train, val = One_hot_encoder(train, val, variable=['Education_Status', 'Employment_Status',\n",
    "                                                    'Industry_Status', 'Occupation_Status', 'Race',\n",
    "                                                    'Hispanic_Origin', 'Martial_Status', 'Household_Status',\n",
    "                                                    'Household_Summary', 'Citizenship', 'Birth_Country',\n",
    "                                                    'Birth_Country (Father)', 'Birth_Country (Mother)', 'Tax_Status', 'Income_Status'])\n",
    "\n",
    "train['Gender'] = np.where(train['Gender'] == 'M', 1, 0)\n",
    "val['Gender'] = np.where(val['Gender'] == 'M', 1, 0)\n",
    "\n",
    "train = train.drop(['ID'], axis = 1)\n",
    "val = val.drop(['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Education_Status', 'Employment_Status',\n",
    "            'Industry_Status', 'Occupation_Status', 'Race',\n",
    "            'Hispanic_Origin', 'Martial_Status', 'Household_Status',\n",
    "            'Household_Summary', 'Citizenship', 'Birth_Country',\n",
    "            'Birth_Country (Father)', 'Birth_Country (Mother)', 'Tax_Status', 'Income_Status'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = train.columns.astype(str)\n",
    "val.columns = val.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop('Income', axis = 1), train['Income']\n",
    "X_val, y_val = val.drop('Income', axis = 1), val['Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimension_reduction(train, *args, n_reduced, first_node, second_node):    \n",
    "    \n",
    "    input_layer = Input(shape=(train.shape[1],))\n",
    "    encoded = Dense(first_node)(input_layer)\n",
    "    encoded = LeakyReLU()(encoded) \n",
    "    encoded = Dense(second_node)(encoded)\n",
    "    encoded = LeakyReLU()(encoded) \n",
    "    \n",
    "    encoded = Dense(n_reduced)(encoded)\n",
    "    encoded = LeakyReLU()(encoded) \n",
    "    \n",
    "    decoded = Dense(second_node)(encoded)\n",
    "    decoded = LeakyReLU()(decoded) \n",
    "    decoded = Dense(first_node)(decoded)\n",
    "    decoded = LeakyReLU()(decoded) \n",
    "    output_layer = Dense(train.shape[1], activation='elu')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    autoencoder.fit(train, train, epochs=50, batch_size=16, verbose = False)\n",
    "\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    column_names = list(string.ascii_lowercase)[:n_reduced]    \n",
    "    train_reduced = pd.DataFrame(encoder.predict(train, verbose = False), columns=column_names)\n",
    "\n",
    "    train = pd.concat([train, train_reduced], axis = 1)\n",
    "    \n",
    "    print('Reduced Col N :', len(train_reduced.columns))\n",
    "\n",
    "    encoded_args = list()    \n",
    "    if args:\n",
    "        for data in args:\n",
    "\n",
    "            data_reduced = pd.DataFrame(encoder.predict(data, verbose = False), columns=column_names)\n",
    "\n",
    "            data = pd.concat([data, data_reduced], axis = 1)\n",
    "        \n",
    "            encoded_args.append(data)\n",
    "    \n",
    "    return [train, *encoded_args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = Dimension_reduction(X_train, X_val, n_reduced=5, first_node=256, second_node=128, method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(boosting_type = 'gbdt', learning_rate = 0.01, n_estimators=300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUMIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
